{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "from utils import progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {}\n",
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "test_loss_history = []\n",
    "test_acc_history = []\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "best_param = 0\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_full = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.Caltech101(\n",
    "    root='../data', target_type='category', download=True, transform=transform_full)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model = timm.create_model('seresnet34', num_classes=101)\n",
    "pretrain_model = pretrain_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(pretrain_model.parameters(), lr=4e-4, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch, net, net_name, FMCE=False):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        if FMCE:\n",
    "            outputs,feature_maps = net(inputs)\n",
    "        elif FMCE == False and net_name.startswith('DueHeadNet'):\n",
    "            outputs,_ = net(inputs)\n",
    "        else:\n",
    "            outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        if FMCE:\n",
    "            feature_maps_a, feature_maps_b = feature_maps[0], feature_maps[1]\n",
    "            feature_maps_loss = F.cosine_similarity(feature_maps_b,feature_maps_a, dim=1).mean()\n",
    "            loss += feature_maps_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        # progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        #              % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        print(f\"Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f} ({correct}/{total})\", end='\\r')\n",
    "        \n",
    "    train_loss_history.append(train_loss / len(trainloader))\n",
    "    train_acc_history.append(100.*correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, net, net_name, FMCE=False):\n",
    "    global best_acc\n",
    "    global best_param\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            if FMCE:\n",
    "                outputs, feature_maps = net(inputs)\n",
    "            elif FMCE == False and net_name.startswith('DueHeadNet'):\n",
    "                outputs,_ = net(inputs)\n",
    "            else:\n",
    "                outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            if FMCE:\n",
    "                feature_maps_a, feature_maps_b = feature_maps[0], feature_maps[1]\n",
    "                feature_maps_loss = F.cosine_similarity(feature_maps_b,feature_maps_a, dim=1).mean()\n",
    "                loss += feature_maps_loss\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            # progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            #              % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "            print(f\"Loss: {test_loss/(batch_idx+1)} | Acc: {100.*correct/total} ({correct}/{total})\", end='\\r')\n",
    "            \n",
    "    test_loss_history.append(test_loss / len(testloader))\n",
    "    test_acc_history.append(100.*correct/total)\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print()\n",
    "        print('New best model found!')\n",
    "        best_acc = acc\n",
    "        best_param = net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, start_epoch+20):\n",
    "    print()\n",
    "    train(epoch, pretrain_model, 'seresnet34')\n",
    "    print()\n",
    "    test(epoch, pretrain_model, 'seresnet34')\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model.load_state_dict(best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DueHeadNet(nn.Module):\n",
    "    def __init__(self, num_classes=1011, base_model=\"seresnet34\", pretrain_model=None):\n",
    "        super(DueHeadNet, self).__init__()\n",
    "        self.pretrain_model = pretrain_model\n",
    "        self.model2 = timm.create_model(base_model, num_classes=num_classes)\n",
    "        self.feature_table = {\n",
    "            \"seresnet18\": 512*7*7,\n",
    "            \"seresnet34\": 512*7*7,\n",
    "            \"seresnet50\": 2048*7*7,\n",
    "            \"seresnet101\": 2048*7*7,\n",
    "            \"seresnet152\": 2048*7*7\n",
    "        }\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(self.feature_table[base_model], 512),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feature_maps_list = []\n",
    "        with torch.no_grad():\n",
    "            fe_map_a = self.pretrain_model.forward_features(x)\n",
    "        fe_map_b = self.model2.forward_features(x)\n",
    "        feature_maps_list.append(fe_map_a)\n",
    "        feature_maps_list.append(fe_map_b)\n",
    "        # feature_maps = torch.stack(feature_maps_list, dim=1)\n",
    "        feature_maps = fe_map_b + fe_map_a\n",
    "        feature_maps = feature_maps.view(feature_maps.size(0), -1)\n",
    "        logits = self.cls(feature_maps)\n",
    "        return logits, feature_maps_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = DueHeadNet(num_classes=101, base_model='seresnet34', pretrain_model=pretrain_model)\n",
    "new_model = new_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(new_model.parameters(), lr=4e-4, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+45):\n",
    "    print()\n",
    "    train(epoch, new_model, 'DueHeadNet(seresnet34)', FMCE=True)\n",
    "    print()\n",
    "    test(epoch, new_model, 'DueHeadNet(seresnet34)', FMCE=True)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best accuracy:', best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params comparison\n",
    "print(\"seresnet34 params: \", sum(p.numel() for p in pretrain_model.parameters()))\n",
    "print(\"DueHeadNet params: \", sum(p.numel() for p in new_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['DueHeadNet(seresnet34)'] = {\n",
    "    'best_acc': best_acc,\n",
    "    'parms': sum(p.numel() for p in new_model.parameters())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_FMCE = DueHeadNet(num_classes=101, base_model='seresnet34', pretrain_model=pretrain_model)\n",
    "new_model_FMCE = new_model_FMCE.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(new_model_FMCE.parameters(), lr=4e-4, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
    "\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch+45):\n",
    "    print()\n",
    "    train(epoch, new_model_FMCE, 'DueHeadNet(seresnet34)NFMCE', FMCE=False)\n",
    "    print()\n",
    "    test(epoch, new_model_FMCE, 'DueHeadNet(seresnet34)NFMCE', FMCE=False)\n",
    "    scheduler.step()\n",
    "\n",
    "print('Best accuracy:', best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['DueHeadNet(seresnet34)NFMCE'] = {\n",
    "    'best_acc': best_acc,\n",
    "    'parms': sum(p.numel() for p in new_model.parameters())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a single seresnet101 to compare\n",
    "\n",
    "seresnet34 = timm.create_model('seresnet50', num_classes=101)\n",
    "seresnet34 = seresnet34.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(seresnet34.parameters(), lr=4e-4, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=60)    \n",
    "best_acc = 0\n",
    "for epoch in range(start_epoch, start_epoch+30):\n",
    "    print()\n",
    "    train(epoch, seresnet34, 'seresnet50')\n",
    "    print()\n",
    "    test(epoch, seresnet34, 'seresnet50')\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"seresnet50 params: \", sum(p.numel() for p in seresnet34.parameters()))\n",
    "print(best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['seresnet50'] = {\n",
    "    'best_acc': best_acc,\n",
    "    'parms': sum(p.numel() for p in seresnet34.parameters())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('info.json', 'w') as f:\n",
    "    json.dump(info, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
